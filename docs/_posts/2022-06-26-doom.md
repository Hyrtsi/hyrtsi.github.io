---
layout: post
title:  "Playing FPS games using Reinforcement Learning"
date:   2022-06-26 13:37:00 +0200
tags: games ML programming research
---

## Playing FPS games using Reinforcement Learning

I like programming, math, machine learning and video games. Apparently reinforcement learning combines all of these. This is a series of posts of my journey in developing some kickass machine learning algorithms with the end goal of crushing Doom2 with my AI.

![Doom2]({{site.baseurl}}/assets/doom2.jpg)

## Goal

Doom2 is immensely difficult to master so we are starting simple.

- Playing from raw pixel input
- Using autogenerated easy maps and handcrafted maps at first
  - Maps that have no enemies
  - Maps with consistent theme
  - No secrets, teleporters, only exit
- Learning navigation first

I'm using [vizdoom](https://github.com/mwydmuch/ViZDoom), a AI research platform for Doom2 machine learning.

![Vizdoom](https://camo.githubusercontent.com/a7d9d95fc80903bcb476c2bbdeac3fa7623953c05401db79101c2468b0d90ad9/687474703a2f2f7777772e63732e7075742e706f7a6e616e2e706c2f6d6b656d706b612f6d6973632f76697a646f6f6d5f676966732f76697a646f6f6d5f636f727269646f725f7365676d656e746174696f6e2e676966)

### Long term goals
- To learn reinforcement learning and computer vision
- To have fun
- To create an agent that learns novel Doom strategies
- To create an agent that is able to play vanilla Doom2 maps relatively well

## Research

Vanilla Q-learning cannot be applied to continuous action or state spaces. However, using function approximation with a nonlinear function approximator such as a neural network instead of the Q-table one can apply Q-learning to continuous domain. However, Q-learning needs to employ the temporal dimension to fully learn FPS games. [This](https://arxiv.org/abs/1609.05521) paper uses Deep Recurrent Q-Networks (DQRN) to tackle the problems that require time-awareness and memory to perform well.

The authors have noticed the following problems:
- Delayed rewards
- Long sequences
- Few rewards

The agent may be rewarded a variable time after its actions. It will usually take a long time for the agent to reach its destination (the exit of the level). If reaching the exit is the only source for reward it will take the agent a high number of trials before it is rewarded and thus learning is slow. [Lample and Chaplot](https://arxiv.org/abs/1609.05521) use reward shaping and [Wu and Tian](https://openreview.net/pdf?id=Hk3mPK5gg) use [curriculum learning](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/) to tackle this problem. While these are good solutions they are not easily generalizeable to other tasks.

[Song et al.](https://www.ijcai.org/proceedings/2019/0482.pdf) have created an environment-aware hierarchical method that uses a manager-worker-model.

There are numerous continuous-domain methods such as
- [Asynchronous Advantage Actor Critic](https://arxiv.org/abs/1602.01783v2) ([A3C](https://paperswithcode.com/method/a3c))
- [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971v6) ([DDPG](https://keras.io/examples/rl/ddpg_pendulum/))
- [Normalized Advantage Functions](https://github.com/carpedm20/NAF-tensorflow) ([NAF](https://arxiv.org/abs/1603.00748))
- [Twin Delayed Deep Deterministic Policy Gradient](https://arxiv.org/abs/1802.09477v3) (TD3)
- [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) ([PPO](https://arxiv.org/abs/1707.06347))

## Domain

FPS games have continuous state and action space. In Doom2 each frame contains

$$320*200*255*255*255 = 10^{12}$$

combinations for the pixels of the screen (fraction of which can ever be seen in the game). However, there are significantly more states considering the 2D locations of the game objects (player, enemies, projectiles, items) using floating point accuracy. In addition to this, the player health, state of game objectives such as opened doors, picked up items, used powerups, weapon choice and others increase the complixity.

There are 38 binary and 5 continuous controls in [vizdoom](https://github.com/mwydmuch/ViZDoom/blob/master/doc/Types.md#button). For navigation at least the following are needed:

- Move forward, left, right (backward not needed)
- Turn right/left $$\theta$$ degrees (continuous, can be quantized)
- Use

For combat we need in addition:

- Attack, alternative attack
- Reload
- select weapon 0-9
- activate selected item
- select next/previous item

That gives us in total 21 actions. Thus, we need a function that takes in `320 x 200 x 3` frames and returns a vector of length `21` with floating point numbers in range $$[0 ... 1]$$ that represent the probability for binary action and the magnitude of a continuous action.

That's good for starters.

It's possible to reward the agent easily in vizdoom for
- living
- reaching the exit (= episode ends early)
- player reaching high velocity
- player reaching a certain `(x,y,z)` location in the world
- player finding enemies (= visible on screen)
- player finding ammo / health / armor / keys (= visible on screen)
- killcount
- item count
- secret count
- hit/damage count
- health, armor, ammo possessed
- changing the weapon

and give negative reward for
- hits/damage taken
- dying
- not moving

We start by creating simple small maps with no enemies and only an exit. We try to teach the AI to find the exit. It is rewarded for moving and reaching the exit. The maximum episode length must be really high and the last N steps of the episode are stored in ring buffer and used for training.

<!---
### For starters

Computers have been invented to mimic human behavior and automate tasks for us. The AI research has been around since the 1950s but it skyrocketed in the 2010s because the availability of data and powerful hardware. You can see some of the major milestones in machine learning [here](https://en.wikipedia.org/wiki/Timeline_of_machine_learning) and AI [here](https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence) if you want some perspective.

[Reinforcement learning](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=dp_ob_title_bk) is one of the three basic machine learning paradigms alongside with supervised learning and unsupervised learning. The agents take actions and get rewards for their actions. This kind of learning resembles video games or how we teach our dogs to fetch us things.

[This](https://arxiv.org/abs/1312.5602) paper was an important one in reinforcement learning. The model learns to play Atari games such as pong from raw pixel data. We fast forward a few years and we get [AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago): an AI that plays the ancient game of go at superhuman level and beats the world champion in the game. Just a few years after that, [AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go): AI that plays not only go but also other board games at super-human level without any human training data by just playing against itself. Finally, in 2020, we get [MuZero](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules) that beats all the other algorithms in go, chess, shogi, atari and other games without knowing the rules of the game beforehand.

I heard just recently of [PPO](https://openai.com/blog/openai-baselines-ppo/) and [PPG](https://arxiv.org/abs/2009.04416) and I think I could continue this list forever. We have to celebrate our amazing AI research that is definitely going towards a general AI that

- doesn't need to know the rules of the game beforehand
- does not need human-generated data to learn but learns from trial and error
- handles the huge dimensionality of the state space
- can play several games using raw sensory data instead of highly pre-processed data
- comes up with creative solutions to problems
- surpasses human players

However, Atari games, chess, shogi and go contain no hidden information. All the players in the games see the full state at one glance. The amount of legal states in a go board is $3^{361}$ or $10^{170}$ if that makes any more sense. Yes, the games require an immense amount of exploration, search, strategy, tactics, memory and so on. But how about this?

FPS games are usually in 3D space (Doom2 is in 2.5D). The player does not see the whole map at once but needs to navigate around the environment. Moreover, the player has only a limited field of vision, for example 90 degrees. Thus, they need to come up with a representation of the surroundings.

Some actions such as shooting require fast reaction time. Some actions require precision. The player must choose between picking up items, finding keys to unlock areas, killing enemies, dodging enemy attacks and switching weapons.

As Doom2 (or FPS games in general) is in continuous-ish space the number of states is a magnitude larger than in discrete board games. All the game objects such as monsters, the player, pickups and projectiles can be in any of the points in the map. In addition, the state of the health, ammunition, current map, pressed buttons and picked up keys increase the complexity of the game.

The temporal dimension is an important difference between Doom2 and board games such as go or chess. There is high correlation between consecutive frames. A human player 
-->