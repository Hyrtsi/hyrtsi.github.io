---
layout: post
title:  "Playing FPS games using Reinforcement Learning"
date:   2022-06-26 13:37:00 +0200
tags: games ml programming research
---

## Playing FPS games using Reinforcement Learning

I like programming, math, machine learning and video games.
Reinforcement learning combines all of these.
This is a series of posts of our journey in developing some kickass machine learning algorithms with the end goal of crushing Doom2.

![Doom2]({{site.baseurl}}/assets/doom2.jpg)

## Goal

Doom2 is immensely difficult to master so we are starting simple.

- Playing from raw pixel input
- Using [autogenerated easy maps](https://github.com/mwydmuch/PyOblige) and handcrafted maps at first
  - Maps that have no enemies
  - Maps with consistent theme
  - No secrets, teleporters, only exit
- Learning navigation first

I'm using [vizdoom](https://github.com/mwydmuch/ViZDoom), a AI research platform for Doom2 machine learning.

![Vizdoom](https://camo.githubusercontent.com/a7d9d95fc80903bcb476c2bbdeac3fa7623953c05401db79101c2468b0d90ad9/687474703a2f2f7777772e63732e7075742e706f7a6e616e2e706c2f6d6b656d706b612f6d6973632f76697a646f6f6d5f676966732f76697a646f6f6d5f636f727269646f725f7365676d656e746174696f6e2e676966)

### Long term goals
- To learn reinforcement learning and computer vision
- To have fun
- To create an agent that learns novel Doom strategies
- To create an agent that is able to play vanilla Doom2 maps relatively well

## Research

There has been research on RL in video game and FPS context for quite a long time.
I summarize the relevant parts from my perspective.

If you're totally new to RL, here are some good starting points:
- [Sutton and Barto: Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
- [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)
- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)

Key research publications in Deep RL:
- [List by OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- [Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. Algorithm: DQN.](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
- [Asynchronous Methods for Deep Reinforcement Learning, Mnih et al, 2016. Algorithm: A3C.](https://arxiv.org/abs/1602.01783)
- [DreamerV3](https://danijar.com/project/dreamerv3/)
- [Muzero](https://arxiv.org/abs/1911.08265) and its predecessor of sorts, [AlphaZero](https://arxiv.org/abs/1712.01815)

Research on Vizdoom platform and related to Doom and FPS games:
- [Song et al.](https://www.ijcai.org/proceedings/2019/0482.pdf) have created an environment-aware hierarchical method that uses a manager-worker-model.
- [Self-Supervised Policy Adaptation during Deployment](https://arxiv.org/abs/2007.04309)
- [Combo-Action](https://ojs.aaai.org/index.php/AAAI/article/view/3885/3763)
- [Playtesting: What is Beyond Personas](https://arxiv.org/abs/2107.11965)
- [Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2111.09858)
- [DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning](https://arxiv.org/abs/1909.02982)
- [Expert-augmented actor-critic for ViZDoom and Montezumaâ€™s Revenge](https://arxiv.org/abs/1809.03447)
- [Autoencoder-augmented Neuroevolution for Visual Doom Playing](https://arxiv.org/abs/1707.03902)
- [Clyde: A deep reinforcement learning DOOM playing agent](https://eprints.whiterose.ac.uk/118807/1/Clyde_A_Deep_RL_Doom_Playing_Agent_Ratcliffe_Devlin_Kruschwitz_Citi.pdf)
- [Automated Curriculum Learning by Rewarding Temporally Rare Events](https://arxiv.org/abs/1803.07131)
- [Deep Reinforcement Learning with VizDoom First-Person Shooter](https://ceur-ws.org/Vol-2479/paper1.pdf)
- [A Survey of Deep Reinforcement Learning in Video Games](https://arxiv.org/abs/1912.10944)
- [ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling](https://arxiv.org/abs/1801.01000)
- [Playing FPS Games with Deep Reinforcement Learning](https://arxiv.org/abs/1609.05521)
- [Learning to Act By Predicting The Future](https://arxiv.org/abs/1611.01779)
- [Building Generalizable Agents with a Realistic and Right 3D Environment](https://arxiv.org/abs/1801.02209)

Other popular methods:
- [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971v6) ([DDPG](https://keras.io/examples/rl/ddpg_pendulum/))
- [Normalized Advantage Functions](https://github.com/carpedm20/NAF-tensorflow) ([NAF](https://arxiv.org/abs/1603.00748))
- [Twin Delayed Deep Deterministic Policy Gradient](https://arxiv.org/abs/1802.09477v3) (TD3)
- [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) ([PPO](https://arxiv.org/abs/1707.06347))


## Approach

The first task: Learn to navigate.

A single frame isn't enough to deduce the player's current trajectory, enemy trajectories, let alone remember where the player has came from to the current room and where they should go.
Thus, we have to think in terms of sequences of frames.

However, storing long sequences isn't going to happen.
There isn't enough memory for 35 frames per second and possibly several minutes of gameplay.

There is a way to tackle this though: compress the frames with an [autoencoder](https://lilianweng.github.io/posts/2018-08-12-vae/).
A compression ratio of 100-500 is possible to achieve. This means we can fit 100-500 times more frames and thus 100-500 times longer sequences into the GPU memory (regardless of what the baseline is).

NN training on GPUs requires a ton of GPU memory.
Not only the input data but also the NN model and the gradients need to fit in the GPU memory during training.
You can read more about that [here](https://huggingface.co/docs/transformers/perf_train_gpu_one) and [here](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/).

We plan to push the training limits on a single RTX4090 GPU and then rent a GPU from [here](http://vast.ai) for the heavy lifting.

---

### Input

Consider a sequence

$$ s = (o_1, a_1, r_1), ..., (o_n, a_n, r_n) $$

where $$o$$ is the observation, $$a$$ action, $$r$$ reward for each timestep respectively.

We want to be able to learn the following things:
- a representation of the observation, `e`, gotten by autoencoder for example
- a dynamics model that learns to predict the next step given current frome and action
- a prediction model that learns the optimal policy and value of the current state 

![This](https://miro.medium.com/max/720/1*NowOwxV5SQ9aLKbjdz41lQ.webp)

This image from the MuZero algorithm sums it up quite well.

The idea of the agent learning the model dynamics is interesting.
It is literally what humans do when they learn new tasks.

The prediction value can be substituted with the Actor-Critic model where there is a model for learning the policy and another for evaluating how good the states attained by the policies are.

We will start with this setting and try to reach small goals one at a time since this task is immensely difficult.

I don't think a single research team has made an agent that plays real Doom2 levels.
If our research project succeeds our model will be able to play Doom, Doom2, Quake, Wolfenstein and literally any FPS game that fulfils certain conditions.
I also think that the fact that there are no models that know how to play Doom means it is immensely difficult.
However, the recent [MineRL](https://minerl.io/) models are promising.

This article will continue in the future as follows:
- I will showcase some simple vizdoom cases
- I will test A3C algorithm on them and show the code + results
- The results will be evaluated and future research ideas will be presented

(continued...)