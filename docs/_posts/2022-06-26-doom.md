---
layout: post
title:  "Playing FPS games using Reinforcement Learning"
date:   2022-06-26 13:37:00 +0200
tags: games ml programming research
---

## Playing FPS games using Reinforcement Learning

I like programming, math, machine learning and video games.
Reinforcement learning combines all of these.
This is a series of posts of our journey in developing some kickass machine learning algorithms with the end goal of crushing Doom2.

![Doom2]({{site.baseurl}}/assets/doom2.jpg)

## Goal

Doom2 is immensely difficult to master so we are starting simple.

- Playing from raw pixel input
- Using [autogenerated easy maps](https://github.com/mwydmuch/PyOblige) and handcrafted maps at first
  - Maps that have no enemies
  - Maps with consistent theme
  - No secrets, teleporters, only exit
- Learning navigation first

I'm using [vizdoom](https://github.com/mwydmuch/ViZDoom), a AI research platform for Doom2 machine learning.

![Vizdoom](https://camo.githubusercontent.com/a7d9d95fc80903bcb476c2bbdeac3fa7623953c05401db79101c2468b0d90ad9/687474703a2f2f7777772e63732e7075742e706f7a6e616e2e706c2f6d6b656d706b612f6d6973632f76697a646f6f6d5f676966732f76697a646f6f6d5f636f727269646f725f7365676d656e746174696f6e2e676966)

### Long term goals
- To learn reinforcement learning and computer vision
- To have fun
- To create an agent that learns novel Doom strategies
- To create an agent that is able to play vanilla Doom2 maps relatively well

## Research

There has been research on RL in video game and FPS context for quite a long time.
I summarize the relevant parts from my perspective.

If you're totally new to RL, here are some good starting points:
- [Sutton and Barto: Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
- [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)
- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)

Key research publications in Deep RL:
- [List by OpenAI](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- [Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. Algorithm: DQN.](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
- [Asynchronous Methods for Deep Reinforcement Learning, Mnih et al, 2016. Algorithm: A3C.](https://arxiv.org/abs/1602.01783)
- [DreamerV3](https://danijar.com/project/dreamerv3/)
- [Muzero](https://arxiv.org/abs/1911.08265) and its predecessor of sorts, [AlphaZero](https://arxiv.org/abs/1712.01815)

Research on Vizdoom platform and related to Doom and FPS games:
- [Song et al.](https://www.ijcai.org/proceedings/2019/0482.pdf) have created an environment-aware hierarchical method that uses a manager-worker-model.
- [Self-Supervised Policy Adaptation during Deployment](https://arxiv.org/abs/2007.04309)
- [Combo-Action](https://ojs.aaai.org/index.php/AAAI/article/view/3885/3763)
- [Playtesting: What is Beyond Personas](https://arxiv.org/abs/2107.11965)
- [Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2111.09858)
- [DRLViz: Understanding Decisions and Memory in Deep Reinforcement Learning](https://arxiv.org/abs/1909.02982)
- [Expert-augmented actor-critic for ViZDoom and Montezumaâ€™s Revenge](https://arxiv.org/abs/1809.03447)
- [Autoencoder-augmented Neuroevolution for Visual Doom Playing](https://arxiv.org/abs/1707.03902)
- [Clyde: A deep reinforcement learning DOOM playing agent](https://eprints.whiterose.ac.uk/118807/1/Clyde_A_Deep_RL_Doom_Playing_Agent_Ratcliffe_Devlin_Kruschwitz_Citi.pdf)
- [Automated Curriculum Learning by Rewarding Temporally Rare Events](https://arxiv.org/abs/1803.07131)
- [Deep Reinforcement Learning with VizDoom First-Person Shooter](https://ceur-ws.org/Vol-2479/paper1.pdf)
- [A Survey of Deep Reinforcement Learning in Video Games](https://arxiv.org/abs/1912.10944)
- [ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling](https://arxiv.org/abs/1801.01000)
- [Playing FPS Games with Deep Reinforcement Learning](https://arxiv.org/abs/1609.05521)
- [Learning to Act By Predicting The Future](https://arxiv.org/abs/1611.01779)
- [Building Generalizable Agents with a Realistic and Right 3D Environment](https://arxiv.org/abs/1801.02209)

Other popular methods:
- [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971v6) ([DDPG](https://keras.io/examples/rl/ddpg_pendulum/))
- [Normalized Advantage Functions](https://github.com/carpedm20/NAF-tensorflow) ([NAF](https://arxiv.org/abs/1603.00748))
- [Twin Delayed Deep Deterministic Policy Gradient](https://arxiv.org/abs/1802.09477v3) (TD3)
- [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) ([PPO](https://arxiv.org/abs/1707.06347))


## Approach

The first task: Learn to navigate.

A single frame isn't enough to deduce the player's current trajectory, enemy trajectories, let alone remember where the player has came from to the current room and where they should go.
Thus, we have to think in terms of sequences of frames.

However, storing long sequences isn't going to happen.
There isn't enough memory for 35 frames per second and possibly several minutes of gameplay.

There is a way to tackle this though: compress the frames with an [autoencoder](https://lilianweng.github.io/posts/2018-08-12-vae/).
A compression ratio of 100-500 is possible to achieve. This means we can fit 100-500 times more frames and thus 100-500 times longer sequences into the GPU memory (regardless of what the baseline is).

NN training on GPUs requires a ton of GPU memory.
Not only the input data but also the NN model and the gradients need to fit in the GPU memory during training.
You can read more about that [here](https://huggingface.co/docs/transformers/perf_train_gpu_one) and [here](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/).

We plan to push the training limits on a single RTX4090 GPU and then rent a GPU from [here](http://vast.ai) for the heavy lifting.

---

### Input

Consider a sequence

$$ s = (o_1, a_1, r_1), ..., (o_n, a_n, r_n) $$

where $$o$$ is the observation, $$a$$ action, $$r$$ reward for each timestep respectively.




(continued...)